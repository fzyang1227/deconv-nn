{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./')\n",
    "from cityscapes import CityScapes\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from shelfnet import ShelfNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helper functions to view network parameters\n",
    "from torchinfo import summary\n",
    "\n",
    "def view_network_shapes(model, input_shape):\n",
    "    print(summary(model, input_size=input_shape))\n",
    "\n",
    "\n",
    "def view_network_parameters(model):\n",
    "    # Visualise the number of parameters\n",
    "    tensor_list = list(model.state_dict().items())\n",
    "    total_parameters = 0\n",
    "    print(\"Model Summary\\n\")\n",
    "    for layer_tensor_name, tensor in tensor_list:\n",
    "        total_parameters += int(torch.numel(tensor))\n",
    "        print(\"{}: {} elements\".format(layer_tensor_name, torch.numel(tensor)))\n",
    "    print(f\"\\nTotal Trainable Parameters: {total_parameters}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 1024, 1024]) torch.Size([4, 1, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# training dataset\n",
    "n_classes = 19\n",
    "batch = 4\n",
    "n_workers = 4\n",
    "cropsize = [1024, 1024] # [h, w] of the cropped image\n",
    "ds_train = CityScapes('data/', cropsize=cropsize, mode='train')\n",
    "dl_train = DataLoader(ds_train,\n",
    "                batch_size = batch,\n",
    "                shuffle = True,\n",
    "                num_workers = n_workers,\n",
    "                pin_memory = True,\n",
    "                drop_last = True)\n",
    "\n",
    "# iterate through the dataset\n",
    "for i, (imgs, label) in enumerate(dl_train):\n",
    "    print(imgs.shape, label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to simplify the model even further by reducing the channels more\n",
    "\n",
    "Below is the format of the realtime shelfnet 18\n",
    "\n",
    "```python\n",
    "class ShelfNet(nn.Module):\n",
    "    def __init__(self, n_classes, *args, **kwargs):\n",
    "        super(ShelfNet, self).__init__()\n",
    "        self.backbone = Resnet18()\n",
    "\n",
    "        self.decoder = Decoder(planes=64,layers=3,kernel=3)\n",
    "        self.ladder = LadderBlock(planes=64,layers=3, kernel=3)\n",
    "\n",
    "        self.conv_out = NetOutput(64, 64, n_classes)\n",
    "        self.conv_out16 = NetOutput(128, 64, n_classes)\n",
    "        self.conv_out32 = NetOutput(256, 64, n_classes)\n",
    "\n",
    "        self.trans1 = ConvBNReLU(128,64,ks=1,stride=1,padding=0)\n",
    "        self.trans2 = ConvBNReLU(256, 128, ks=1, stride=1, padding=0)\n",
    "        self.trans3 = ConvBNReLU(512, 256, ks=1, stride=1, padding=0)\n",
    "    def forward(self, x, aux = True):\n",
    "        H, W = x.size()[2:]\n",
    "\n",
    "        feat8, feat16, feat32 = self.backbone(x)\n",
    "\n",
    "        feat8 = self.trans1(feat8)\n",
    "        feat16 = self.trans2(feat16)\n",
    "        feat32 = self.trans3(feat32)\n",
    "\n",
    "        out = self.decoder([feat8, feat16, feat32])\n",
    "\n",
    "        out2 = self.ladder(out)\n",
    "\n",
    "        feat_cp8, feat_cp16, feat_cp32 = out2[-1], out2[-2], out2[-3]\n",
    "\n",
    "        feat_out = self.conv_out(feat_cp8)\n",
    "        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n",
    "\n",
    "        if aux:\n",
    "            feat_out16 = self.conv_out16(feat_cp16)\n",
    "            feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n",
    "\n",
    "            feat_out32 = self.conv_out32(feat_cp32)\n",
    "            feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n",
    "\n",
    "            return feat_out, feat_out16, feat_out32\n",
    "        else:\n",
    "            return feat_out\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
    "        for name, child in self.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params()\n",
    "            if isinstance(child, LadderBlock) or isinstance(child, NetOutput) or isinstance(child, Decoder)\\\n",
    "                    or isinstance(child, ConvBNReLU):\n",
    "                lr_mul_wd_params += child_wd_params\n",
    "                lr_mul_nowd_params += child_nowd_params\n",
    "            else:\n",
    "                wd_params += child_wd_params\n",
    "                nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from resnet2 import resnet18\n",
    "from modules.bn import InPlaceABNSync\n",
    "from ShelfBlock import Decoder, LadderBlock\n",
    "from shelfnet import ConvBNReLU, NetOutput\n",
    "\n",
    "class SimpleShelfNet(nn.Module):\n",
    "    def __init__(self, n_classes, channel_ratio=0.25):\n",
    "        # Add channel_ratio for control of decoder channels\n",
    "        super(SimpleShelfNet, self).__init__()\n",
    "\n",
    "        self.backbone = resnet18()\n",
    "\n",
    "        # Reduce decoder channels by channel_ratio\n",
    "        self.decoder = Decoder(planes=int(64 * channel_ratio), layers=3, kernel=3)\n",
    "        self.ladder = LadderBlock(planes=int(64 * channel_ratio), layers=3, kernel=3)\n",
    "\n",
    "        self.conv_out = NetOutput(int(64 * channel_ratio), 64, n_classes)\n",
    "        self.conv_out16 = NetOutput(int(128 * channel_ratio), 64, n_classes)\n",
    "        self.conv_out32 = NetOutput(int(256 * channel_ratio), 64, n_classes)\n",
    "\n",
    "        # Adjust transition layers if decoder channels are reduced\n",
    "        self.trans1 = ConvBNReLU(128, int(64 * channel_ratio), ks=1, stride=1, padding=0)\n",
    "        self.trans2 = ConvBNReLU(256, int(128 * channel_ratio), ks=1, stride=1, padding=0)\n",
    "        self.trans3 = ConvBNReLU(512, int(256 * channel_ratio), ks=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x, aux = True):\n",
    "        H, W = x.size()[2:]\n",
    "\n",
    "        feat8, feat16, feat32 = self.backbone(x)\n",
    "\n",
    "        feat8 = self.trans1(feat8)\n",
    "        feat16 = self.trans2(feat16)\n",
    "        feat32 = self.trans3(feat32)\n",
    "\n",
    "        out = self.decoder([feat8, feat16, feat32])\n",
    "\n",
    "        out2 = self.ladder(out)\n",
    "\n",
    "        feat_cp8, feat_cp16, feat_cp32 = out2[-1], out2[-2], out2[-3]\n",
    "\n",
    "        feat_out = self.conv_out(feat_cp8)\n",
    "        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n",
    "\n",
    "        if aux:\n",
    "            feat_out16 = self.conv_out16(feat_cp16)\n",
    "            feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n",
    "\n",
    "            feat_out32 = self.conv_out32(feat_cp32)\n",
    "            feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n",
    "\n",
    "            return feat_out, feat_out16, feat_out32\n",
    "        else:\n",
    "            return feat_out\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
    "        for name, child in self.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params()\n",
    "            if isinstance(child, LadderBlock) or isinstance(child, NetOutput) or isinstance(child, Decoder)\\\n",
    "                    or isinstance(child, ConvBNReLU):\n",
    "                lr_mul_wd_params += child_wd_params\n",
    "                lr_mul_nowd_params += child_nowd_params\n",
    "            else:\n",
    "                wd_params += child_wd_params\n",
    "                nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "SimpleShelfNet                                     [1, 19, 1024, 1024]       --\n",
      "├─ResNet: 1-1                                      [1, 128, 128, 128]        513,000\n",
      "│    └─Conv2d: 2-1                                 [1, 64, 512, 512]         9,408\n",
      "│    └─BatchNorm2d: 2-2                            [1, 64, 512, 512]         128\n",
      "│    └─MaxPool2d: 2-3                              [1, 64, 256, 256]         --\n",
      "│    └─Sequential: 2-4                             [1, 64, 256, 256]         --\n",
      "│    │    └─BasicBlock: 3-1                        [1, 64, 256, 256]         73,984\n",
      "│    │    └─BasicBlock: 3-2                        [1, 64, 256, 256]         73,984\n",
      "│    └─Sequential: 2-5                             [1, 128, 128, 128]        --\n",
      "│    │    └─BasicBlock: 3-3                        [1, 128, 128, 128]        230,144\n",
      "│    │    └─BasicBlock: 3-4                        [1, 128, 128, 128]        295,424\n",
      "│    └─Sequential: 2-6                             [1, 256, 64, 64]          --\n",
      "│    │    └─BasicBlock: 3-5                        [1, 256, 64, 64]          919,040\n",
      "│    │    └─BasicBlock: 3-6                        [1, 256, 64, 64]          1,180,672\n",
      "│    └─Sequential: 2-7                             [1, 512, 32, 32]          --\n",
      "│    │    └─BasicBlock: 3-7                        [1, 512, 32, 32]          3,673,088\n",
      "│    │    └─BasicBlock: 3-8                        [1, 512, 32, 32]          4,720,640\n",
      "├─ConvBNReLU: 1-2                                  [1, 16, 128, 128]         --\n",
      "│    └─Conv2d: 2-8                                 [1, 16, 128, 128]         2,048\n",
      "│    └─BatchNorm2d: 2-9                            [1, 16, 128, 128]         32\n",
      "├─ConvBNReLU: 1-3                                  [1, 32, 64, 64]           --\n",
      "│    └─Conv2d: 2-10                                [1, 32, 64, 64]           8,192\n",
      "│    └─BatchNorm2d: 2-11                           [1, 32, 64, 64]           64\n",
      "├─ConvBNReLU: 1-4                                  [1, 64, 32, 32]           --\n",
      "│    └─Conv2d: 2-12                                [1, 64, 32, 32]           32,768\n",
      "│    └─BatchNorm2d: 2-13                           [1, 64, 32, 32]           128\n",
      "├─Decoder: 1-5                                     [1, 64, 32, 32]           2,384\n",
      "│    └─BasicBlock: 2-14                            [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-9                            [1, 64, 32, 32]           36,928\n",
      "│    │    └─InPlaceABNSync: 3-10                   [1, 64, 32, 32]           128\n",
      "│    │    └─Dropout2d: 3-11                        [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-12                           [1, 64, 32, 32]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-13                      [1, 64, 32, 32]           128\n",
      "│    └─ModuleList: 2-17                            --                        (recursive)\n",
      "│    │    └─AttentionRefinementModule: 3-14        [1, 32, 32, 32]           19,584\n",
      "│    └─ModuleList: 2-18                            --                        (recursive)\n",
      "│    │    └─ConvBNReLU: 3-15                       [1, 32, 64, 64]           9,280\n",
      "│    └─ModuleList: 2-17                            --                        (recursive)\n",
      "│    │    └─AttentionRefinementModule: 3-16        [1, 16, 64, 64]           4,928\n",
      "│    └─ModuleList: 2-18                            --                        (recursive)\n",
      "│    │    └─ConvBNReLU: 3-17                       [1, 16, 128, 128]         2,336\n",
      "├─LadderBlock: 1-6                                 [1, 64, 32, 32]           --\n",
      "│    └─BasicBlock: 2-19                            [1, 16, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-18                           [1, 16, 128, 128]         2,320\n",
      "│    │    └─InPlaceABNSync: 3-19                   [1, 16, 128, 128]         32\n",
      "│    │    └─Dropout2d: 3-20                        [1, 16, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-21                           [1, 16, 128, 128]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-22                      [1, 16, 128, 128]         32\n",
      "│    └─ModuleList: 2-22                            --                        (recursive)\n",
      "│    │    └─BasicBlock: 3-23                       [1, 16, 128, 128]         2,384\n",
      "│    └─ModuleList: 2-23                            --                        (recursive)\n",
      "│    │    └─Conv2d: 3-24                           [1, 32, 64, 64]           4,640\n",
      "│    └─ModuleList: 2-22                            --                        (recursive)\n",
      "│    │    └─BasicBlock: 3-25                       [1, 32, 64, 64]           9,376\n",
      "│    └─ModuleList: 2-23                            --                        (recursive)\n",
      "│    │    └─Conv2d: 3-26                           [1, 64, 32, 32]           18,496\n",
      "│    └─BasicBlock: 2-24                            [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-27                           [1, 64, 32, 32]           36,928\n",
      "│    │    └─InPlaceABNSync: 3-28                   [1, 64, 32, 32]           128\n",
      "│    │    └─Dropout2d: 3-29                        [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-30                           [1, 64, 32, 32]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-31                      [1, 64, 32, 32]           128\n",
      "│    └─ModuleList: 2-27                            --                        (recursive)\n",
      "│    │    └─AttentionRefinementModule: 3-32        [1, 32, 32, 32]           19,584\n",
      "│    └─ModuleList: 2-28                            --                        (recursive)\n",
      "│    │    └─ConvBNReLU: 3-33                       [1, 32, 64, 64]           9,280\n",
      "│    └─ModuleList: 2-27                            --                        (recursive)\n",
      "│    │    └─AttentionRefinementModule: 3-34        [1, 16, 64, 64]           4,928\n",
      "│    └─ModuleList: 2-28                            --                        (recursive)\n",
      "│    │    └─ConvBNReLU: 3-35                       [1, 16, 128, 128]         2,336\n",
      "├─NetOutput: 1-7                                   [1, 19, 128, 128]         --\n",
      "│    └─ConvBNReLU: 2-29                            [1, 64, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-36                           [1, 64, 128, 128]         9,216\n",
      "│    │    └─BatchNorm2d: 3-37                      [1, 64, 128, 128]         128\n",
      "│    └─Conv2d: 2-30                                [1, 19, 128, 128]         10,944\n",
      "├─NetOutput: 1-8                                   [1, 19, 64, 64]           --\n",
      "│    └─ConvBNReLU: 2-31                            [1, 64, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-38                           [1, 64, 64, 64]           18,432\n",
      "│    │    └─BatchNorm2d: 3-39                      [1, 64, 64, 64]           128\n",
      "│    └─Conv2d: 2-32                                [1, 19, 64, 64]           10,944\n",
      "├─NetOutput: 1-9                                   [1, 19, 32, 32]           --\n",
      "│    └─ConvBNReLU: 2-33                            [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-40                           [1, 64, 32, 32]           36,864\n",
      "│    │    └─BatchNorm2d: 3-41                      [1, 64, 32, 32]           128\n",
      "│    └─Conv2d: 2-34                                [1, 19, 32, 32]           10,944\n",
      "====================================================================================================\n",
      "Total params: 12,016,760\n",
      "Trainable params: 12,016,760\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 39.14\n",
      "====================================================================================================\n",
      "Input size (MB): 12.58\n",
      "Forward/backward pass size (MB): 905.57\n",
      "Params size (MB): 46.01\n",
      "Estimated Total Size (MB): 964.16\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# original realtime model\n",
    "simple_model = SimpleShelfNet(n_classes=n_classes, channel_ratio=.25).cuda()\n",
    "\n",
    "view_network_shapes(simple_model, torch.Size([1, 3, cropsize[0], cropsize[1]])) # batch size is 8, but see pass through of 1 image at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean running time is  0.011665486700003384\n"
     ]
    }
   ],
   "source": [
    "# load weights\n",
    "simple_model.eval()\n",
    "run_time = []\n",
    "\n",
    "# measure inference time via random input\n",
    "for i in range(0,100):\n",
    "    input = torch.randn(1,3,1024,1024).cuda()\n",
    "    # ensure that context initialization and normal_() operations\n",
    "    # finish before you start measuring time\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = simple_model(input , aux=False)\n",
    "\n",
    "    torch.cuda.synchronize()  # wait for mm to finish\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    run_time.append(end-start)\n",
    "\n",
    "print('Mean running time is ', np.mean(run_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a modified version of their training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "it: 10/1000, loss: 8.7320, eta: 0:12:13, time: 6.6570\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "it: 20/1000, loss: 7.2930, eta: 0:09:48, time: 4.7428\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "it: 30/1000, loss: 7.0740, eta: 0:09:00, time: 4.7576\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n",
      "losses calculated\n",
      "backwards step done\n",
      "data loaded\n",
      "outputs done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m\n\u001b[1;32m     45\u001b[0m out, out16, out32 \u001b[38;5;241m=\u001b[39m net(im)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs done\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m lossp \u001b[38;5;241m=\u001b[39m \u001b[43mLossP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m Loss2(out16, lb)\n\u001b[1;32m     51\u001b[0m loss3 \u001b[38;5;241m=\u001b[39m Loss3(out32, lb)\n",
      "File \u001b[0;32m~/repos/deconv-nn/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/deconv-nn/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/deconv-nn/ShelfNet18_realtime/./loss.py:28\u001b[0m, in \u001b[0;36mOhemCELoss.forward\u001b[0;34m(self, logits, labels)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_min]\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from loss import OhemCELoss\n",
    "from optimizer import Optimizer\n",
    "\n",
    "respth = './res'\n",
    "\n",
    "## model\n",
    "ignore_idx = 255\n",
    "net = SimpleShelfNet(n_classes=n_classes, channel_ratio=.25)\n",
    "net.cuda()\n",
    "net.train()\n",
    "score_thres = 0.7\n",
    "n_min = batch*cropsize[0]*cropsize[1]//16\n",
    "LossP = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "Loss2 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "Loss3 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "\n",
    "## optimizer\n",
    "max_iter = 80000\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 5e-4\n",
    "optim = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "## train loop\n",
    "msg_iter = 50\n",
    "loss_avg = []\n",
    "st = glob_st = time.time()\n",
    "diter = iter(dl_train)\n",
    "epoch = 0\n",
    "for it in range(max_iter):\n",
    "    im, lb = next(diter)\n",
    "\n",
    "    if not im.size()[0]==batch:\n",
    "        epoch += 1\n",
    "        diter = iter(dl_train)\n",
    "        im, lb = next(diter)\n",
    "        continue\n",
    "\n",
    "    im = im.cuda()\n",
    "    lb = lb.cuda()\n",
    "    H, W = im.size()[2:]\n",
    "    lb = torch.squeeze(lb, 1)\n",
    "\n",
    "    optim.zero_grad()\n",
    "    out, out16, out32 = net(im)\n",
    "    lossp = LossP(out, lb)\n",
    "    loss2 = Loss2(out16, lb)\n",
    "    loss3 = Loss3(out32, lb)\n",
    "    loss = lossp + loss2 + loss3\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "\n",
    "    loss_avg.append(loss.item())\n",
    "\n",
    "    ## print training log message\n",
    "    if (it+1)%msg_iter==0:\n",
    "        loss_avg = sum(loss_avg) / len(loss_avg)\n",
    "        ed = time.time()\n",
    "        t_intv, glob_t_intv = ed - st, ed - glob_st\n",
    "        eta = int((max_iter - it) * (glob_t_intv / it))\n",
    "        eta = str(datetime.timedelta(seconds=eta))\n",
    "        msg = ', '.join([\n",
    "                'it: {it}/{max_it}',\n",
    "                'loss: {loss:.4f}',\n",
    "                'eta: {eta}',\n",
    "                'time: {time:.4f}',\n",
    "            ]).format(\n",
    "                it = it+1,\n",
    "                max_it = max_iter,\n",
    "                loss = loss_avg,\n",
    "                time = t_intv,\n",
    "                eta = eta\n",
    "            )\n",
    "        print(msg)\n",
    "        loss_avg = []\n",
    "        st = ed\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        ## dump the models in between\n",
    "        save_pth = osp.join(respth, 'shelfnet_model_it_%d.pth'%it)\n",
    "        torch.save(net.state_dict(), save_pth)\n",
    "\n",
    "## dump the final model\n",
    "save_pth = osp.join(respth, 'model_final.pth')\n",
    "net.cpu()\n",
    "state = net.module.state_dict() if hasattr(net, 'module') else net.state_dict()\n",
    "torch.save(state, save_pth)\n",
    "print('training done, model saved to: {}'.format(save_pth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# validation dataset\n",
    "batchsize = 2\n",
    "n_workers = 2\n",
    "ds_val = CityScapes('data/', mode='val')\n",
    "dl_val = DataLoader(ds_val,\n",
    "                batch_size = batchsize,\n",
    "                shuffle = False,\n",
    "                num_workers = n_workers,\n",
    "                drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134/134 [01:21<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mIoU: 0.004969619019889352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluate import MscEval\n",
    "\n",
    "# loading pretrained weights\n",
    "net = SimpleShelfNet(n_classes=19, channel_ratio=.25)\n",
    "net.load_state_dict(torch.load('res/model_final.pth'))\n",
    "net.cuda()\n",
    "net.eval()\n",
    "\n",
    "# evaluate via mIoU\n",
    "mEval = MscEval(net, dl_val, flip=False)\n",
    "mIoU, hist = mEval.evaluate()\n",
    "print(f'mIoU: {mIoU}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
